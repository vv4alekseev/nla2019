{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"feature_extraction_vgg.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"wlhTDlmkMFoS","colab_type":"code","outputId":"df6fb09b-22dd-4d2e-dcf5-cdf0682f3afe","executionInfo":{"status":"ok","timestamp":1576729961943,"user_tz":-180,"elapsed":2237,"user":{"displayName":"Vladislav Alekseev","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBPZCZHjoQRoJkzonvm3ko52phLOzEjSjF1RZYSQg=s64","userId":"12790138709720723860"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from __future__ import print_function\n","from __future__ import division\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["PyTorch Version:  1.3.1\n","Torchvision Version:  0.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZWQTzJ5obsVw","colab_type":"code","outputId":"b75a20ac-aaaa-470f-b204-3b0fd969cc15","executionInfo":{"status":"ok","timestamp":1576729980029,"user_tz":-180,"elapsed":17170,"user":{"displayName":"Vladislav Alekseev","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBPZCZHjoQRoJkzonvm3ko52phLOzEjSjF1RZYSQg=s64","userId":"12790138709720723860"}},"colab":{"base_uri":"https://localhost:8080/","height":131}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7ZbQlvArQXv-","colab_type":"code","colab":{}},"source":["# Top level data directory. Here we assume the format of the directory conforms\n","# to the ImageFolder structure\n","data_dir = \"/content/drive/My Drive/Colab Notebooks/NLA/data/dset_ssvd_8_2\"\n","\n","# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n","model_name = \"vgg\"\n","\n","# Number of classes in the dataset\n","num_classes = 2\n","\n","# Batch size for training (change depending on how much memory you have)\n","batch_size = 8\n","\n","# Number of epochs to train for\n","num_epochs = 15\n","\n","# Flag for feature extracting. When False, we finetune the whole model,\n","#   when True we only update the reshaped layer params\n","feature_extract = True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u7a0XagNQX1a","colab_type":"code","colab":{}},"source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    if is_inception and phase == 'train':\n","                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCs3zz7pQX4I","colab_type":"code","colab":{}},"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9thxzywMQX6u","colab_type":"code","outputId":"700d295d-1e69-4f6d-d752-c981632cda3c","executionInfo":{"status":"ok","timestamp":1576729998453,"user_tz":-180,"elapsed":8025,"user":{"displayName":"Vladislav Alekseev","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBPZCZHjoQRoJkzonvm3ko52phLOzEjSjF1RZYSQg=s64","userId":"12790138709720723860"}},"colab":{"base_uri":"https://localhost:8080/","height":859}},"source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18\n","        \"\"\"\n","        model_ft = models.resnet18(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = models.alexnet(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"squeezenet\":\n","        \"\"\" Squeezenet\n","        \"\"\"\n","        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n","        model_ft.num_classes = num_classes\n","        input_size = 224\n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","\n","    return model_ft, input_size\n","\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","\n","# Print the model we just instantiated\n","print(model_ft)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg11_bn-6002323d.pth\" to /root/.cache/torch/checkpoints/vgg11_bn-6002323d.pth\n","100%|██████████| 507M/507M [00:05<00:00, 97.7MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["VGG(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU(inplace=True)\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU(inplace=True)\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU(inplace=True)\n","    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (13): ReLU(inplace=True)\n","    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (17): ReLU(inplace=True)\n","    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (20): ReLU(inplace=True)\n","    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (22): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (23): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (24): ReLU(inplace=True)\n","    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (26): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (27): ReLU(inplace=True)\n","    (28): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n","  (classifier): Sequential(\n","    (0): Linear(in_features=25088, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=2, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"goDEhF4wQX9J","colab_type":"code","outputId":"54cff0d7-c960-4854-e3b0-ce6359d74614","executionInfo":{"status":"ok","timestamp":1576730002222,"user_tz":-180,"elapsed":10608,"user":{"displayName":"Vladislav Alekseev","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBPZCZHjoQRoJkzonvm3ko52phLOzEjSjF1RZYSQg=s64","userId":"12790138709720723860"}},"colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["# Data augmentation and normalization for training\n","# Just normalization for validation\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.RandomResizedCrop(input_size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.CenterCrop(input_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ]),\n","}\n","\n","print(\"Initializing Datasets and Dataloaders...\")\n","\n","# Create training and validation datasets\n","image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n","# Create training and validation dataloaders\n","dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n","\n","# Detect if we have a GPU available\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Initializing Datasets and Dataloaders...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sYIsp84pQX_5","colab_type":"code","outputId":"4bb224f6-def1-423d-e448-8281f9e375e5","executionInfo":{"status":"ok","timestamp":1576730007728,"user_tz":-180,"elapsed":15145,"user":{"displayName":"Vladislav Alekseev","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBPZCZHjoQRoJkzonvm3ko52phLOzEjSjF1RZYSQg=s64","userId":"12790138709720723860"}},"colab":{"base_uri":"https://localhost:8080/","height":73}},"source":["# Send the model to GPU\n","model_ft = model_ft.to(device)\n","\n","# Gather the parameters to be optimized/updated in this run. If we are\n","#  finetuning we will be updating all parameters. However, if we are\n","#  doing feature extract method, we will only update the parameters\n","#  that we have just initialized, i.e. the parameters with requires_grad\n","#  is True.\n","params_to_update = model_ft.parameters()\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Params to learn:\n","\t classifier.6.weight\n","\t classifier.6.bias\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lS3Cyr_qQYCN","colab_type":"code","outputId":"5b95b470-05e4-41a9-a9d7-29a10cc14d1c","executionInfo":{"status":"ok","timestamp":1576730099290,"user_tz":-180,"elapsed":105913,"user":{"displayName":"Vladislav Alekseev","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBPZCZHjoQRoJkzonvm3ko52phLOzEjSjF1RZYSQg=s64","userId":"12790138709720723860"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Setup the loss fxn\n","criterion = nn.CrossEntropyLoss()\n","\n","# Train and evaluate\n","model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Epoch 0/14\n","----------\n","train Loss: 0.6031 Acc: 0.6747\n","val Loss: 0.4370 Acc: 0.8427\n","\n","Epoch 1/14\n","----------\n","train Loss: 0.4837 Acc: 0.7769\n","val Loss: 0.3963 Acc: 0.8548\n","\n","Epoch 2/14\n","----------\n","train Loss: 0.4982 Acc: 0.7661\n","val Loss: 0.3784 Acc: 0.8427\n","\n","Epoch 3/14\n","----------\n","train Loss: 0.5190 Acc: 0.7473\n","val Loss: 0.3814 Acc: 0.8508\n","\n","Epoch 4/14\n","----------\n","train Loss: 0.5193 Acc: 0.7715\n","val Loss: 0.3751 Acc: 0.8508\n","\n","Epoch 5/14\n","----------\n","train Loss: 0.5607 Acc: 0.7258\n","val Loss: 0.3855 Acc: 0.8226\n","\n","Epoch 6/14\n","----------\n","train Loss: 0.5040 Acc: 0.7634\n","val Loss: 0.3790 Acc: 0.8266\n","\n","Epoch 7/14\n","----------\n","train Loss: 0.5179 Acc: 0.7742\n","val Loss: 0.3889 Acc: 0.8145\n","\n","Epoch 8/14\n","----------\n","train Loss: 0.5591 Acc: 0.7473\n","val Loss: 0.3814 Acc: 0.8306\n","\n","Epoch 9/14\n","----------\n","train Loss: 0.4795 Acc: 0.7688\n","val Loss: 0.3830 Acc: 0.8347\n","\n","Epoch 10/14\n","----------\n","train Loss: 0.4434 Acc: 0.7957\n","val Loss: 0.3563 Acc: 0.8548\n","\n","Epoch 11/14\n","----------\n","train Loss: 0.5225 Acc: 0.7823\n","val Loss: 0.4369 Acc: 0.7984\n","\n","Epoch 12/14\n","----------\n","train Loss: 0.5099 Acc: 0.7554\n","val Loss: 0.4014 Acc: 0.8105\n","\n","Epoch 13/14\n","----------\n","train Loss: 0.4992 Acc: 0.8065\n","val Loss: 0.3779 Acc: 0.8468\n","\n","Epoch 14/14\n","----------\n","train Loss: 0.4597 Acc: 0.7769\n","val Loss: 0.3853 Acc: 0.8145\n","\n","Training complete in 1m 32s\n","Best val Acc: 0.854839\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BIMnzXpMbmAj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}